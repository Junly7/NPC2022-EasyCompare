\section{GUC DSL}
\label{sec:gnndsl}
In this section, we propose a DSL called GUC (\underline{G}NN \underline{U}nified Language based on \underline{C}$++$) to define GNN models. GUC DSL is based on LLVM Clang~\cite{clang2022} and in accordance with the C$++$ syntax. The GUC form is close to the Python interface of GNN frameworks. Meanwhile, we simplify the details of defining GNNs to improve the usability of GUC. Next, we introduce the code block and GUC APIs, as well as an example of GCN implementation using GUC.

\subsection{Code Block}
\label{subsec:codeblock}
GUC uses the code block to define various GNN models. The code block is transformed into executable Python codes on DGL or PyG (discussed in Section \ref{sec:codetran}). Figure~\ref{fig:gcn_guc} shows an example of code block for network definition in GUC. The code block consists of three fundamental parts:

\textbf{(a) Function definition.} Define every function that will be initialized later such as GNN convolutions, activations and dropouts.

\textbf{(b) Function initialization.} Initialize the functions with appropriate parameters (e.g., hidden length). The \textsl{in\_feat} and \textsl{out\_feat} are determined by the dataset without instantiation.

\textbf{(c) Forward propagation definition.} Define the forward propagation with input feature tensor \textsl{h} and functions initialized above. The output tensor is returned after passing through the functions.

\begin{figure}
  \begin{lstlisting}
class NET {
  // (a) Function definition
  Conv conv1, conv2;
  Activation relu1, relu2;

  void __init__(int in_feat,int out_feat) {
    super();  // Corresponding to "super().__init__()"
    // (b) Function initialization
    conv1=GCNConv(in_feat,16,true,true);
    conv2=GCNConv(16,out_feat,false,true);
    relu1=ReLU();
    relu2=LeakyReLU(0.5);
  }

  tensor forward(tensor h) {
    // (c) Forward propagation definition
    h=relu1.in(conv1.in(h));
    h=relu2.in(conv2.in(h));
    return h;
  }
};
  \end{lstlisting}
  \caption{GCN implementation in GUC.}
  \label{fig:gcn_guc}
\end{figure}

\subsection{GUC APIs}
\label{subsec:nnapis}

The main functional interfaces of GUC include graph convolutions, activation functions and mini-batch. In addition, GUC also involves other interfaces to implement network definition and batch training (e.g. tensor, dropout and linear).

%\textbf{GNN Convolution.}
\subsubsection{Graph Convolution} Currently, GUC defines four graph convolution interfaces including \textsl{GCNConv}, \textsl{SAGEConv}, \textsl{GATConv} and \textsl{GINConv}, corresponding to GCN, GraphSAGE, GAT and GIN respectively. Each interface has at least a initialization function for layer definition and a forward propagation function for tensor processing. The forward propagation function is defined as follows:

\begin{center}
\textbf{\textsl{Conv.in}}:$(init:W, f:X\rightarrow Y, in: X)\rightarrow Y$
\end{center}

\noindent The forward propagation function takes the feature tensor and weight matrix as input and obtains the output tensor through arithmetic calculation. The graph convolution definitions will be transformed into functionally equivalent interfaces on different GNN frameworks.

%\textbf{Activation Function.}
\subsubsection{Activation Function} The activation function interfaces in GUC include \textsl{ReLU}, \textsl{LeakyReLU}, \textsl{Tanh}, \textsl{ELU}, etc. Similar to graph convolution interfaces, each activation function interface is with a tensor processing function:

\begin{center}
\textbf{\textsl{Activation.in}}:$([init:op], f:X\rightarrow Y, in: X)\rightarrow Y$
\end{center}

\noindent The tensor processing function applies non-linear activations to the input tensor. The activation function definitions will be transformed to corresponding interfaces in PyTorch backend.

\subsection{An Example: GCN Implementation}
\label{subsec:gcn_guc}
Figure \ref{fig:gcn_guc} shows an example of GCN implementation in GUC. The model consists of two GCN layers, each nested within an activation function. The initialization interfaces of \textsl{GCNConv} and \textsl{LeakyReLU} are defined as follows:

\begin{center}
\textbf{\textsl{GCNConv}}:$({\rm int}\; in\_feat, {\rm int}\; out\_feat, {\rm bool}\; bias, {\rm bool}\; normalize)$ \\
\textbf{\textsl{LeakyReLU}}:$({\rm double}\; negative\_slope)$
\end{center}
