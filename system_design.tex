\section{System Design}
\label{sec:system}

% 突出学术上的贡献，淡化系统的工程实现；详尽说明系统支持的功能

We design a fully automated integration system EasyCompare, which supports the functions of model transformation, deployment training, and visualization of multi-dimensional analysis. In the industry, OctoML~\cite{octoml2022} aims to provide performance comparisons and deployment costs for model execution on specific hardware to enrich user experience. However, OctoML focuses on traditional models implemented by DL frameworks without support for GNNs. In contrast, EasyCompare is committed to facilitating the code transformation and framework comparison of GNNs. Furthermore, EasyCompare also provides performance and cost-efficiency analysis of different cloud GPUs, allowing users to make informed platform choices based on their needs.

\subsection{GPU Resource Scheduling}

To utilize richer computing resources, EasyCompare supports access to both local and remote GPUs. EasyCompare can directly allocate resources to training tasks for local GPUs after obtaining permission. After task execution, EasyCompare collects the result logs and enters the analysis phase.

EasyCompare needs to unify the runtime environment for remote GPUs and ensure data consistency during task execution. Before training, the master host with EasyCompare deployment sends the training files (GNN model after code transformation) to the slave host via SSH File Transfer Protocol (SFTP). During the training process, the master host remotely executes the training files via SSH and redirects the runtime logs to the local. After training, EasyCompare sends valid result logs back to the master host via SFTP. Figure~\ref{fig:run_on_remote_server} shows the flow of task running on a remote GPU.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/running_remote_server.pdf}
    \caption{The flow of task running on a remote GPU.}
    \label{fig:run_on_remote_server}
    \vspace{-0.7cm}
\end{figure}

\subsection{Performance Metric Collection}


\subsubsection{GPU Utilization Calculation} During task training, EasyCompare utilizes the NVIDIA System Management Interface (\textsl{nvidia-smi}) to collect GPU utilization statistics periodically. The command example is \textsl{nvidia-smi --query --gpu=utilization.gpu --format=csv --loop=1}. After task completion, the statistics are averaged as the overall GPU utilization for the training task.

\subsubsection{Cost Efficiency Comparison} EasyCompare makes it easy to compare the performance of GNNs implemented by different frameworks deployed to diverse GPUs. On the other hand, cost-efficiency (performance per dollar) can help users make trade-offs between cost and performance. Cost-efficiency is defined as the cost of training the same number of epochs, and the cost is often proportional to the training duration. Therefore, the formula for cost efficiency is defined as \textsl{CE=ES/cost}, where \textsl{ES} is number of epochs per second, and \textsl{cost} is determined by the cloud service provider. Furthermore, EasyCompare also gives the accurate comparisons for secondary reference.

\subsection{System Function Synthesis}

As mentioned above, EasyCompare integrates three main functions: code generation, deployment training, and visualization of multi-dimensional analysis. EasyCompare adopts the form of front-end and back-end separation. The front-end is responsible for user interaction and result visualization, whereas the back-end realizes the system functions with control calculation logic.


\subsubsection{Code Generation \& Deployment Training} EasyCompare passes the model description written in GUC to the back-end code transformation tool, which translates the description into GNN model codes implemented by DGL and PyG interfaces. Before execution, the user can select the graph dataset, the number of epochs, and GPU architecture for training through front-end interaction. After collecting the above information, the back-end automatically generates executable files on the specified host and starts training. The console display during training (e.g., changes in training accuracy) is fed back to the front-end for real-time monitoring.

\begin{figure}[!ht]
    \centering
    \begin{minipage}[c]{0.35\textwidth}
        \centering
        \includegraphics[width=1\textwidth]{images/pie_op_compare.jpg}
        \subcaption{\centering Operator comparison with pie charts}
        \label{fig:pie_op_compare}
    \end{minipage}
    \begin{minipage}[c]{0.47\textwidth}
        \includegraphics[width=1\textwidth]{images/same_op_compare.png}
        \subcaption{\centering Comparative analysis of identical operators}
        \label{fig:same_op_compare}
    \end{minipage}
    \caption{Visualization of operator comparisons between GNN frameworks.}
    \label{fig:result_comparison}
    \vspace{-0.6cm}
\end{figure}


\textbf{Visualization of Multi-dimensional Analysis} EasyCompare collects detailed statistics during model training. These statistics are exploited to calculate GPU utilization, conduct multi-dimensional performance comparisons, and analyze operator execution details. EasyCompare utilizes PyTorch Profiler to obtain the time breakdown of each operator occupying hardware resources in the training process. EasyCompare presents the operator comparison between DGL and PyG with pie charts (shown in Figure~\ref{fig:pie_op_compare}). Further, EasyCompare provides a comparative analysis of the identical operators in the two graphs (shown in Figure~\ref{fig:same_op_compare}). By doing so, researchers can dig deeper into how the frameworks behave when dealing with the same model.