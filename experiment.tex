\section{Evaluation (3 pages)}
\label{sec:evaluation}

\subsection{Experiment Setup}
\label{sec:setup}

% ƫС�����ݼ�������GPU�ŵ��£�(6-8��)��full graph training���������ݼ���1-2������mini-batch training
\subsubsection{Hardware and software configurations}
We conduct evaluation on two servers equipped with two types of GPUs, as shown in Table \ref{tab:The GPUs used for evaluation}.
The rental fee of GPUs is based on Google Cloud Server~\cite{googlecloud2022}.
Both of the server are equipped with E5-2680 v4 @ 2.4Hz, 28 cores and 256 GB main memory.

\begin{table}
    \centering
    \vspace{-1cm} %调整与上文的垂直距离
    \caption{The GPUs used for evaluation}
    \label{tab:The GPUs used for evaluation}
    \setlength{\tabcolsep}{7.mm}{
        \begin{tabular}{|c|c|c|c|}
            \hline
            GPU & Mem. & TFLOPS & Rental \\
            \hline
            V100 & 16GB HBM2 & 7.8 & 2.48\$ per hour \\
            A100 & 40GB HBM2 & 9.7 & 2.93\$ per hour \\
            \hline
        \end{tabular}
    }
    \vspace{-1cm}
\end{table}



\subsubsection{GNN configurations and graph datasets}
Four typical GNN models introduced above, GAT, GCN, GIN and GraphSAGE, are used in the experiment. The GNN models both have 2 layers of graph convolution with ReLU activation function, transformed by code transformer with GUC DSL, maintaining functional equivalence between frameworks.
As shown in Table \ref{tab:Datasets used for evaluation}, the experiments involves four scales of graph datasets: small, medium, large and oversized, to improve the universality of comparison results.

\begin{table}
    \vspace{-0.8cm} %调整与上文的垂直距离
    \centering
    \caption{Datasets used for evaluation}
    \label{tab:Datasets used for evaluation}
    \setlength{\tabcolsep}{2.mm}{
        \begin{tabular}{|c|c|c|c|c|c|}
            \hline
            Dataset & \#Nodes & \#Edges & Feature Length & \#Classes & Scale\\
            \hline
            PubMed & 19,717 & 99,203 & 500 & 3 & small \\
            PPI & 56,944 & 818,716 & 50 & 121 & small \\
            arXiv & 169,343 & 1,166,243 & 128 & 40 & middle \\
            DD & 334,925 & 1,686,092 & 89 & 32 & middle \\
            COLLAB & 235,868 & 2,358,104 & 128 & 32 & middle \\
            PPA & 576,289 & 42,463,862 & 58 & 16 & large \\
            PROTEINS & 132,534 & 79,122,504 & 8 & 2 & large \\
            reddit.dgl & 232,965 & 114,615,891 & 602 & 50 & oversized \\
            Products & 2,449,029 & 123,718,280 & 100 & 47 & oversized \\
            \hline
        \end{tabular}
    }
    \vspace{-0.7cm} %调整与下文的垂直距离
\end{table}

\subsubsection{Comparison metrics}
The key metrics for evaluation are time overhead, GPU utilization and peak memory consumption which will be measured through training multiple datasets under four GNNs.
The analysis of operators focuses on the decomposition of operators' proportion which is the metric of operator breakdown.
Based on time overhead, a case study on cost-efficiency under different GPUs will compare the pure performance and cost-efficiency.

\subsection{Framework Comparison}
\label{sec:frameworkcomp}
% ע��˵����EasyCompare�ſ�����������֮���Ĺ����ԱȺͿ��ӻ��������û���������ѡ�������ܷ���
% (on V100) Compare PyG with DGL, including execution time, GPU utilization, memory usage ... 3 * 4 figures
% Analyse the reasons of experiment results, from the perspective of framework design (e.g., graph operator implementation)
\begin{figure}
    \vspace{-0.8cm} %调整图片与上文的垂直距离
    \centering

    \begin{minipage}[c]{1\textwidth}

        \includegraphics[width=\textwidth]{images/full_graph_memory.pdf}
        \subcaption{Peak memory consumption.}
        \label{fig:full_graph_memory}


        \includegraphics[width=\textwidth]{images/full_graph_time.pdf}
        \subcaption{Exection time.}
        \label{fig:full_graph_time}

        \includegraphics[width=\textwidth]{images/full_graph_gpu.pdf}
        \subcaption{GPU utilization.}
        \label{fig:full_graph_gpu}

    \end{minipage}

    \caption{Full-graph training of 128 epoches on V100, OOM means out-of-memory.}
    \label{fig:full_graph}

    \vspace{-1cm} %调整图片与下文的垂直距离
\end{figure}

\subsubsection{Full-graph Training}
As shown in Figure \ref{fig:full_graph}, in full-graph training, we measure several training metrics on GPU of 128 epoches.
For execution time, DGL is slower by a small margin (8\%-15\%, except GAT in which DGL gets 1.2$\times$ faster) in training the small and medium-sized datasets which is due to framework overhead~\cite{wang2019deep}.
For large datasets, DGL is 1.45-2.2$\times$ faster than PyG, because DGL's GSpMM operator (shown in Figure \ref{fig:op_breakdown_dgl}) avoids generating message tensors while PyG's scatter-gather operator does no avoidance (shown in Figure \ref{fig:op_breakdown_pyg}).
It also explains why PyG runs out-of-memory in GAT on large datasets due to PPA and PROTEINS are denser and have more node features.

For peak memory consumption, it is obvious that DGL has a more powerful memory management for GNNs that PyG consumes about 2-6$\times$ more memory than DGL.
DGL manages to keep a low memory footprint due to its GSpMM operator fusing the message computation with aggregation.

For GPU utilization, PyG occupies GPU more frequently and completely than DGL.
PyG achieve 1.2-1.9x of more GPU Utilization.
A deeper look at the GPU utilization curve shows that the two curves almost coincide with each other from 0 to 100\% during training.
However, because DGL often finishes training faster, the utilization will be lower when calculating the average value.



\subsubsection{Mini-batch Training} %(on V100) Compare PyG with DGL, only execution time, 1 * 4 figures
Figure \ref{fig:minibatch_time} evaluates only the execution time of mini-batch training.
We use neighbor sampling (NS) and set mini-batch size to 10240 to train 8 epoches to obtain the measurement data.
The total execution time also depends on the cost of sample preparation, including sampling operations and data movement from CPU to GPU, compared to full graph training.
For NS, sample generation and data movement can occupy up to 85\% of the total training time while GPU Utilization hovers between 0 and 30\% which accounts for the significant performance degradation.

\begin{figure}
    \vspace{-0.5cm} %调整图片与上文的垂直距离
    \centering
    \includegraphics[width=\textwidth]{images/minibatch_time.pdf}
    \caption{Mini-batch training of 8 epoches on V100, only shows execution time, OOM means out-of-memory.}
    \label{fig:minibatch_time}
    \vspace{-1cm} %调整图片与下文的垂直距离
\end{figure}


\subsection{Operator Breakdown}
\label{sec:op_breakdown}
% (on V100) PyG, DGL, Top-10/20 operator intesection (4-6 operators each figure), 2 * 4 figures
For the DGL and PyG, we counted the proportion of operators in their respective training process and selected the top 5 operators respectively for horizontal comparison.
As shown in Figure \ref{fig:op_breakdown}, DGL has obvious characteristics in selecting operators.
For small and medium-sized data sets, the occupancy of GSpMM operators is about 20\%, and for denser datasets, it reaches 60\%-80\%.
The top 3 operators of DGL account for almost 60\% or even more than 80\%.
The operator proportion of PyG is relatively dispersed and PyG shows more scatter-gather operators while dealing with denser datasets.
Due to the gigantic real world graph, DGL developed fused message passing technique and consolidated it with sparse matrix computation into generalized sparse-dense matrix multiplication (g-SpMM) in order to reducing memory traffic.
It accounts for that DGL can achieves much more better performance (in execution time and peak memory consumption) in denser and more edge features datasets.

In addition, we found that training in GAT performs worse than other GNNs (i.e. high peak memory consumption, execution time and dispersive ratio of operators) in Figure \ref{fig:full_graph} and Figure \ref{fig:op_breakdown}.
\textsl{GATConv} scales bad by design, as autograd needs to hold tensors of shape \textsl{[num\_edges, num\_heads, 2*num\_features]} in memory.
There is no real solution to prevent this, as we need to compute attention scores for each edge and each head, based on pair-wise node features.
All conv operators will scale up by edges if they depend on pair-wise node features or edge features.
However, in \textsl{GCNConv}, it can be implemented much more memory-friendly as this operator does not require mapping node features into edge space, e.g., by using sparse-matrix multiplications.

\begin{figure}
    \vspace{-0.5cm} %调整图片与上文的垂直距离
    \centering
    \begin{minipage}[c]{1\textwidth}
        \includegraphics[width=\textwidth]{images/op_breakdown_dgl.pdf}
        \subcaption{Operator breakdown of DGL.}
        \label{fig:op_breakdown_dgl}

        \includegraphics[width=\textwidth]{images/op_breakdown_pyg.pdf}
        \subcaption{Operator breakdown of PyG.}
        \label{fig:op_breakdown_pyg}
    \end{minipage}
    \caption{Operator breakdown of training profile on V100, 128 epoches, OOM means out-of-memory.}
    \label{fig:op_breakdown}
    \vspace{-1cm} %调整图片与下文的垂直距离
\end{figure}

\subsection{Case Study: Cloud GPU Selection}
\label{sec:casestudy}
% ע��˵����EasyCompare���Է�����������ͬGPU�����ܶԱȣ������û�����GPUѡ��
% (only full-graph training) Compare V100 with A100, considering pure performance and cost efficiency, 2 * 4 figures
% Analyse the reasons of experiment results, from the perspective of hardware details, computational characteristics of GNNs
Currently, most researchers consider renting cloud GPU servers to run deep learning tasks.
This case study will simply explain the computing characteristics of GNNs and the selection of cloud servers based on the metric results of two different GPU in Table \ref{tab:The GPUs used for evaluation}.

\begin{figure}
    \vspace{-0.8cm} %调整图片与上文的垂直距离
    \centering
    \begin{minipage}[c]{1\textwidth}
        \includegraphics[width=\textwidth]{images/pure_performance.pdf}
        \subcaption{Pure performance.}
        \label{fig:pure_performance}

        \includegraphics[width=\textwidth]{images/cost_efficiency.pdf}
        \subcaption{Cost-efficiency analysis.}
        \label{fig:cost_efficiency}
    \end{minipage}
    \caption{128 epoches of training on V100 and A100 based on PROTEINS.}
    \label{fig:case_study}
    \vspace{-0.8cm} %调整图片与下文的垂直距离
\end{figure}

\subsubsection{Pure Performance}
Figure \ref{fig:pure_performance} shows the pure performance of V100 and A100 based on PROTEINS (There is no comparison of small or medium-sized datasets because they almost perform the same and except GAT because of PyG's out-of-memory).
A100 is 1.2$\times$ faster than V100 on average which confirms improvement of GPU TFLOPS.
Due to the computing characteristics of GNN are memory intensive, GPU utilization on V100 is lower than that on A100.


\subsubsection{Cost Efficiency}
Based on the pure performance and the rental price in Table \ref{tab:The GPUs used for evaluation}.
We present the cost-efficiency of the two GPUs.
Figure \ref{fig:cost_efficiency} shows the ground truth on training GNN considering cost-efficiency.
The GPU utilization is also present at the top of each bar.
It shows that the two GPUs perform different considering each GNN framework.
DGL on V100 performs better than A100 while PyG on A100 performs better than V100.
